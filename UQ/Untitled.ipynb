{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Name: Tesla V100-PCIE-32GB\n",
      "GPU Memory Total: 32.00 GB\n",
      "GPU Memory Free: 0.02 GB\n",
      "GPU Memory Allocated: 0.02 GB\n",
      "GPU Memory Total: 32.00 GB\n",
      "GPU Memory Free: 1.13 GB\n",
      "GPU Memory Allocated: 1.13 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if a GPU is available and set it as the default device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Print information about the GPU\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory Total: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\n",
    "    print(f\"GPU Memory Free: {torch.cuda.memory_allocated(0) / (1024**3):.2f} GB\")\n",
    "    print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0) / (1024**3):.2f} GB\")\n",
    "\n",
    "# Run a simple GPU computation to ensure GPU resources are active\n",
    "x = torch.randn(10000, 10000).to(device)\n",
    "y = torch.randn(10000, 10000).to(device)\n",
    "z = torch.mm(x, y)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"GPU Memory Total: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\n",
    "    print(f\"GPU Memory Free: {torch.cuda.memory_allocated(0) / (1024**3):.2f} GB\")\n",
    "    print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0) / (1024**3):.2f} GB\")\n",
    "\n",
    "# Verify GPU usage in Task Manager or GPU monitoring tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU : 0.1070241928100586\n",
      "CPU : 4.648635387420654\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "x = torch.randn(10000, 10000).to(device)\n",
    "y = torch.randn(10000, 10000).to(device)\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "z = torch.mm(x, y)\n",
    "\n",
    "print('GPU :',time.time()-t_start)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "x = torch.randn(10000, 10000).to(device)\n",
    "y = torch.randn(10000, 10000).to(device)\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "z = torch.mm(x, y)\n",
    "\n",
    "print('CPU :',time.time()-t_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nvidia_smi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnvidia_smi\u001b[39;00m\n\u001b[0;32m      2\u001b[0m nvidia_smi\u001b[38;5;241m.\u001b[39mnvmlInit()\n\u001b[0;32m      3\u001b[0m deviceCount \u001b[38;5;241m=\u001b[39m nvidia_smi\u001b[38;5;241m.\u001b[39mnvmlDeviceGetCount()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nvidia_smi'"
     ]
    }
   ],
   "source": [
    "import nvidia_smi\n",
    "nvidia_smi.nvmlInit()\n",
    "deviceCount = nvidia_smi.nvmlDeviceGetCount()\n",
    "for i in range(deviceCount):\n",
    "    handle = nvidia_smi.nvmlDeviceGetHandleByIndex(i)\n",
    "    util = nvidia_smi.nvmlDeviceGetUtilizationRates(handle)\n",
    "    mem = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"|Device {i}| Mem Free: {mem.free/1024**2:5.2f}MB / {mem.total/1024**2:5.2f}MB | gpu-util: {util.gpu:3.1%} | gpu-mem: {util.memory:3.1%} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nvidia-ml-py3\n",
      "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "Building wheels for collected packages: nvidia-ml-py3\n",
      "  Building wheel for nvidia-ml-py3 (setup.py): started\n",
      "  Building wheel for nvidia-ml-py3 (setup.py): finished with status 'done'\n",
      "  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19196 sha256=142cb15f51f3aa10facb2d1e04ef826701570aebf440b439c6c2479a9d07b686\n",
      "  Stored in directory: c:\\users\\u0150568\\appdata\\local\\pip\\cache\\wheels\\b9\\b1\\68\\cb4feab29709d4155310d29a421389665dcab9eb3b679b527b\n",
      "Successfully built nvidia-ml-py3\n",
      "Installing collected packages: nvidia-ml-py3\n",
      "Successfully installed nvidia-ml-py3-7.352.0\n"
     ]
    }
   ],
   "source": [
    "!pip install nvidia-ml-py3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gpustat in c:\\users\\u0150568\\.conda\\envs\\torch-cuda\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: psutil>=5.6.0 in c:\\users\\u0150568\\.conda\\envs\\torch-cuda\\lib\\site-packages (from gpustat) (5.9.0)\n",
      "Requirement already satisfied: nvidia-ml-py>=11.450.129 in c:\\users\\u0150568\\.conda\\envs\\torch-cuda\\lib\\site-packages (from gpustat) (12.535.133)\n",
      "Requirement already satisfied: blessed>=1.17.1 in c:\\users\\u0150568\\.conda\\envs\\torch-cuda\\lib\\site-packages (from gpustat) (1.20.0)\n",
      "Requirement already satisfied: jinxed>=1.1.0 in c:\\users\\u0150568\\.conda\\envs\\torch-cuda\\lib\\site-packages (from blessed>=1.17.1->gpustat) (1.2.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\u0150568\\.conda\\envs\\torch-cuda\\lib\\site-packages (from blessed>=1.17.1->gpustat) (1.16.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in c:\\users\\u0150568\\.conda\\envs\\torch-cuda\\lib\\site-packages (from blessed>=1.17.1->gpustat) (0.2.5)\n",
      "Requirement already satisfied: ansicon in c:\\users\\u0150568\\.conda\\envs\\torch-cuda\\lib\\site-packages (from jinxed>=1.1.0->blessed>=1.17.1->gpustat) (1.89.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gpustat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgpustat\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m gpustat \u001b[38;5;241m-\u001b[39m\u001b[43mcp\u001b[49m \u001b[38;5;241m-\u001b[39mi\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cp' is not defined"
     ]
    }
   ],
   "source": [
    "import gpustat\n",
    "gpustat -cp -i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-cuda",
   "language": "python",
   "name": "torch-cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
