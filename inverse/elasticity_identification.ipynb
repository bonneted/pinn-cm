{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'BVP_function' from 'c:\\\\Users\\\\u0150568\\\\OneDrive - KU Leuven\\\\Code\\\\public github\\\\pinn-cm\\\\inverse\\\\BVP_function.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import BVP_function as bvp\n",
    "import numpy as np\n",
    "import deepxde as dde\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\") # better looking plots\n",
    "\n",
    "print(\"Using GPU:\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "\n",
    "from importlib import reload\n",
    "reload(bvp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(bvp)\n",
    "\n",
    "lmbd = 1.0\n",
    "lmbd_trainable = dde.Variable(lmbd - 0.2)\n",
    "mu = 0.5\n",
    "mu_trainable = dde.Variable(mu + 0.2)\n",
    "Q = 4.0\n",
    "\n",
    "domain = np.array([[0.0, 1.0], [0.0, 1.0]])\n",
    "geom = dde.geometry.Rectangle([0, 0], [1, 1])\n",
    "\n",
    "phy_params = {'lmbd': lmbd, 'mu': mu, 'Q': Q}\n",
    "phy_params_trainable = {'lmbd': lmbd_trainable, 'mu': mu_trainable, 'Q': Q}\n",
    "\n",
    "#net parameters\n",
    "net_type = ['Unet','USnet'][1]\n",
    "n_layers = 4\n",
    "size_layers = 50\n",
    "activation = ['tanh','ReLU','Sigmoid'][0]\n",
    "\n",
    "#loss parameters\n",
    "loss_type = ['pde','energy'][0] #the energy loss appears to crash LBFGS and provide less accurate results with Adam. PDE loss is therefore used in the following\n",
    "num_domain = 50**2\n",
    "train_distribution = ['uniform','pseudo','LHS','Halton','Hammersley','Sobol'][4]\n",
    "\n",
    "#boundary conditions\n",
    "bc_type = ['soft','hard'][1]\n",
    "num_boundary = 50\n",
    "\n",
    "#optimizer\n",
    "optimizers = [\"adam\",\"L-BFGS\"]\n",
    "learning_rates = [1e-3,None]\n",
    "iterations = [3000,None]\n",
    "\n",
    "config = {'net_type':net_type,'n_layers':n_layers,'size_layers':size_layers,'activation':activation,\n",
    "        'loss_type':loss_type,'num_domain':num_domain,'train_distribution':train_distribution,\n",
    "        'bc_type':bc_type,'num_boundary':num_boundary,\n",
    "        'optimizers':optimizers,'learning_rates':learning_rates,'iterations':iterations}  \n",
    "\n",
    "net_exact = bvp.set_exact_solution(net_type, phy_params,lib='torch')\n",
    "\n",
    "model, net_wrong_elasticity, pde_net, energy_net, mat_net = bvp.model_setup(geom, config, phy_params_trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model...\n",
      "'compile' took 0.000423 s\n",
      "\n",
      "Training model...\n",
      "\n",
      "Step      Train loss                        Test loss                         Test metric   \n",
      "0         [2.23e+03, 2.65e+02, 2.10e+03]    [2.22e+03, 2.64e+02, 2.17e+03]    [1.14e+00]    \n",
      "200       [9.84e+02, 9.18e+01, 5.18e+01]    [9.84e+02, 9.07e+01, 4.67e+01]    [9.37e-01]    \n",
      "400       [1.68e+02, 1.33e+01, 2.77e+01]    [1.61e+02, 1.31e+01, 2.62e+01]    [7.02e-01]    \n",
      "600       [4.77e+01, 9.49e+00, 1.73e+01]    [4.48e+01, 8.87e+00, 1.59e+01]    [6.37e-01]    \n",
      "800       [1.92e+01, 4.86e+00, 1.03e+01]    [1.76e+01, 4.34e+00, 9.34e+00]    [6.10e-01]    \n",
      "1000      [9.77e+00, 2.93e+00, 6.80e+00]    [8.66e+00, 2.49e+00, 6.05e+00]    [5.99e-01]    \n",
      "1200      [5.84e+00, 1.92e+00, 4.79e+00]    [5.02e+00, 1.57e+00, 4.20e+00]    [5.97e-01]    \n",
      "1400      [3.65e+00, 1.30e+00, 3.55e+00]    [3.08e+00, 1.03e+00, 3.07e+00]    [5.97e-01]    \n",
      "1600      [2.36e+00, 9.89e-01, 2.69e+00]    [1.95e+00, 7.64e-01, 2.29e+00]    [5.98e-01]    \n",
      "1800      [1.65e+00, 8.02e-01, 2.05e+00]    [1.34e+00, 6.07e-01, 1.74e+00]    [5.99e-01]    \n",
      "2000      [1.23e+00, 6.68e-01, 1.58e+00]    [9.78e-01, 4.99e-01, 1.33e+00]    [6.00e-01]    \n",
      "2200      [9.42e-01, 5.51e-01, 1.21e+00]    [7.42e-01, 4.06e-01, 1.01e+00]    [6.02e-01]    \n",
      "2400      [7.15e-01, 4.47e-01, 9.32e-01]    [5.54e-01, 3.26e-01, 7.74e-01]    [6.05e-01]    \n",
      "2600      [5.53e-01, 3.69e-01, 7.47e-01]    [4.18e-01, 2.66e-01, 6.16e-01]    [6.07e-01]    \n",
      "2800      [4.34e-01, 3.06e-01, 6.04e-01]    [3.21e-01, 2.18e-01, 4.93e-01]    [6.09e-01]    \n",
      "3000      [3.38e-01, 2.49e-01, 4.90e-01]    [2.46e-01, 1.75e-01, 3.96e-01]    [6.11e-01]    \n",
      "\n",
      "Best model at step 3000:\n",
      "  train loss: 1.08e+00\n",
      "  test loss: 8.17e-01\n",
      "  test metric: [6.11e-01]\n",
      "\n",
      "'train' took 223.283191 s\n",
      "\n",
      "Compiling model...\n",
      "'compile' took 0.000926 s\n",
      "\n",
      "Training model...\n",
      "\n",
      "Step      Train loss                        Test loss                         Test metric   \n",
      "3000      [3.38e-01, 2.49e-01, 4.90e-01]    [2.46e-01, 1.75e-01, 3.96e-01]    [6.11e-01]    \n",
      "4000      [1.43e-07, 9.28e-08, 4.45e-08]    [1.08e-07, 8.16e-08, 3.71e-08]    [1.00e+00]    \n",
      "5000      [7.81e-08, 6.00e-08, 2.50e-08]    [5.95e-08, 4.93e-08, 2.30e-08]    [1.00e+00]    \n",
      "6000      [4.70e-08, 3.23e-08, 2.12e-08]    [3.69e-08, 2.67e-08, 1.94e-08]    [1.00e+00]    \n",
      "7000      [3.73e-08, 1.54e-08, 1.03e-08]    [2.84e-08, 1.23e-08, 9.44e-09]    [1.01e+00]    \n",
      "8000      [3.57e-08, 1.47e-08, 9.79e-09]    [2.71e-08, 1.16e-08, 8.88e-09]    [1.01e+00]    \n",
      "9000      [3.47e-08, 1.44e-08, 9.44e-09]    [2.63e-08, 1.14e-08, 8.50e-09]    [1.01e+00]    \n",
      "10000     [3.40e-08, 1.42e-08, 9.18e-09]    [2.58e-08, 1.12e-08, 8.23e-09]    [1.01e+00]    \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\u0150568\\OneDrive - KU Leuven\\Code\\public github\\pinn-cm\\inverse\\elasticity_identification.ipynb Cell 3\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/u0150568/OneDrive%20-%20KU%20Leuven/Code/public%20github/pinn-cm/inverse/elasticity_identification.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# # train lbfgs\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/u0150568/OneDrive%20-%20KU%20Leuven/Code/public%20github/pinn-cm/inverse/elasticity_identification.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(\u001b[39m\"\u001b[39m\u001b[39mL-BFGS\u001b[39m\u001b[39m\"\u001b[39m, external_trainable_variables\u001b[39m=\u001b[39mtrainable_variables, metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39ml2 relative error\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/u0150568/OneDrive%20-%20KU%20Leuven/Code/public%20github/pinn-cm/inverse/elasticity_identification.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m losshistory, train_state \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain(callbacks\u001b[39m=\u001b[39;49m[variable])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/u0150568/OneDrive%20-%20KU%20Leuven/Code/public%20github/pinn-cm/inverse/elasticity_identification.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlambda:\u001b[39m\u001b[39m{\u001b[39;00mlmbd_trainable\u001b[39m}\u001b[39;00m\u001b[39m ; mu: \u001b[39m\u001b[39m{\u001b[39;00mlmbd_trainable\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Program Files\\Python38\\lib\\site-packages\\deepxde\\utils\\internal.py:22\u001b[0m, in \u001b[0;36mtiming.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     21\u001b[0m     ts \u001b[39m=\u001b[39m timeit\u001b[39m.\u001b[39mdefault_timer()\n\u001b[1;32m---> 22\u001b[0m     result \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     23\u001b[0m     te \u001b[39m=\u001b[39m timeit\u001b[39m.\u001b[39mdefault_timer()\n\u001b[0;32m     24\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m took \u001b[39m\u001b[39m%f\u001b[39;00m\u001b[39m s\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (f\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, te \u001b[39m-\u001b[39m ts))\n",
      "File \u001b[1;32mc:\\Program Files\\Python38\\lib\\site-packages\\deepxde\\model.py:618\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, iterations, batch_size, display_every, disregard_previous_best, callbacks, model_restore_path, model_save_path, epochs)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_tensorflow_tfp()\n\u001b[0;32m    617\u001b[0m \u001b[39melif\u001b[39;00m backend_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpytorch\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 618\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_pytorch_lbfgs()\n\u001b[0;32m    619\u001b[0m \u001b[39melif\u001b[39;00m backend_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpaddle\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    620\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_paddle_lbfgs()\n",
      "File \u001b[1;32mc:\\Program Files\\Python38\\lib\\site-packages\\deepxde\\model.py:739\u001b[0m, in \u001b[0;36mModel._train_pytorch_lbfgs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    734\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mon_batch_begin()\n\u001b[0;32m    736\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_state\u001b[39m.\u001b[39mset_data_train(\n\u001b[0;32m    737\u001b[0m     \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mtrain_next_batch(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size)\n\u001b[0;32m    738\u001b[0m )\n\u001b[1;32m--> 739\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_step(\n\u001b[0;32m    740\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_state\u001b[39m.\u001b[39;49mX_train,\n\u001b[0;32m    741\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_state\u001b[39m.\u001b[39;49my_train,\n\u001b[0;32m    742\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_state\u001b[39m.\u001b[39;49mtrain_aux_vars,\n\u001b[0;32m    743\u001b[0m )\n\u001b[0;32m    745\u001b[0m n_iter \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mstate_dict()[\u001b[39m\"\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mn_iter\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    746\u001b[0m \u001b[39mif\u001b[39;00m prev_n_iter \u001b[39m==\u001b[39m n_iter:\n\u001b[0;32m    747\u001b[0m     \u001b[39m# Converged\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python38\\lib\\site-packages\\deepxde\\model.py:539\u001b[0m, in \u001b[0;36mModel._train_step\u001b[1;34m(self, inputs, targets, auxiliary_vars)\u001b[0m\n\u001b[0;32m    536\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_step(inputs, targets, auxiliary_vars)\n\u001b[0;32m    537\u001b[0m \u001b[39melif\u001b[39;00m backend_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpytorch\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[39m# TODO: auxiliary_vars\u001b[39;00m\n\u001b[1;32m--> 539\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_step(inputs, targets)\n\u001b[0;32m    540\u001b[0m \u001b[39melif\u001b[39;00m backend_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mjax\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    541\u001b[0m     \u001b[39m# TODO: auxiliary_vars\u001b[39;00m\n\u001b[0;32m    542\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_step(\n\u001b[0;32m    543\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt_state, inputs, targets\n\u001b[0;32m    544\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Program Files\\Python38\\lib\\site-packages\\deepxde\\model.py:341\u001b[0m, in \u001b[0;36mModel._compile_pytorch.<locals>.train_step\u001b[1;34m(inputs, targets)\u001b[0m\n\u001b[0;32m    338\u001b[0m     total_loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m    339\u001b[0m     \u001b[39mreturn\u001b[39;00m total_loss\n\u001b[1;32m--> 341\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mopt\u001b[39m.\u001b[39;49mstep(closure)\n\u001b[0;32m    342\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_scheduler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\optim\\optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\autograd\\grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     27\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[1;32m---> 28\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\optim\\lbfgs.py:437\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[39mif\u001b[39;00m n_iter \u001b[39m!=\u001b[39m max_iter:\n\u001b[0;32m    433\u001b[0m     \u001b[39m# re-evaluate function only if not in last iteration\u001b[39;00m\n\u001b[0;32m    434\u001b[0m     \u001b[39m# the reason we do this: in a stochastic setting,\u001b[39;00m\n\u001b[0;32m    435\u001b[0m     \u001b[39m# no use to re-evaluate that function here\u001b[39;00m\n\u001b[0;32m    436\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[1;32m--> 437\u001b[0m         loss \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(closure())\n\u001b[0;32m    438\u001b[0m     flat_grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gather_flat_grad()\n\u001b[0;32m    439\u001b[0m     opt_cond \u001b[39m=\u001b[39m flat_grad\u001b[39m.\u001b[39mabs()\u001b[39m.\u001b[39mmax() \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m tolerance_grad\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\autograd\\grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     27\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[1;32m---> 28\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Program Files\\Python38\\lib\\site-packages\\deepxde\\model.py:338\u001b[0m, in \u001b[0;36mModel._compile_pytorch.<locals>.train_step.<locals>.closure\u001b[1;34m()\u001b[0m\n\u001b[0;32m    336\u001b[0m total_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(losses)\n\u001b[0;32m    337\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m--> 338\u001b[0m total_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    339\u001b[0m \u001b[39mreturn\u001b[39;00m total_loss\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    300\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    301\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    305\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    306\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 307\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\autograd\\__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m--> 154\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[0;32m    155\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    156\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainable_variables = [lmbd_trainable, mu_trainable]\n",
    "variable = dde.callbacks.VariableValue(trainable_variables, period=100, filename=\"elasticity_param.dat\")\n",
    "\n",
    "model.compile(\"adam\", lr=0.001, external_trainable_variables=trainable_variables, metrics=[\"l2 relative error\"])\n",
    "losshistory, train_state = model.train(iterations=5000,display_every=1000, callbacks=[variable])\n",
    "\n",
    "model.compile(\"adam\", lr=0.0001, external_trainable_variables=trainable_variables, metrics=[\"l2 relative error\"])\n",
    "losshistory, train_state = model.train(iterations=5000,display_every=1000, callbacks=[variable])\n",
    "\n",
    "model.compile(\"adam\", lr=0.0001, external_trainable_variables=trainable_variables, metrics=[\"l2 relative error\"])\n",
    "losshistory, train_state = model.train(iterations=5000,display_every=1000, callbacks=[variable])\n",
    "\n",
    "# # train lbfgs\n",
    "# model.compile(\"L-BFGS\", external_trainable_variables=trainable_variables, metrics=[\"l2 relative error\"])\n",
    "# losshistory, train_state = model.train(callbacks=[variable])\n",
    "print(f\"lambda:{lmbd_trainable} ; mu: {lmbd_trainable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
